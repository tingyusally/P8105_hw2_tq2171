---
title: "p8105_hw2_tq2171"
author: "Tingyu Qian"
date: "2024-09-29"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Problem 1

```{r, message=FALSE}
# read the package
library(tidyverse) 
library(dplyr)
library(tidyr)
library(readxl)
```


```{r, warning=FALSE, message=FALSE}
## read and clean the data
NYC_Transit = read_csv (file = "./NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA", ".")) #read the data
NYC_Transit = janitor::clean_names(NYC_Transit) #clean up variable names after importing data, this will take whatever the column names are and convert them to lower snake case.
```


```{r}
## retain the needed variables
NYC_Transit<-
  select(NYC_Transit, line, station_name, station_latitude, station_longitude, route1:route11, entry, vending, entrance_type, ada)

```

```{r}
## convert the entry variable from character to a logical variable
NYC_Transit$entry <- ifelse(NYC_Transit$entry == "YES", TRUE, FALSE)
```

In the original dataset NYC_Transit, it contains 1868 and 32 columns, which means there are 32 variables. These variables are "Division", "Line", "Station Name", "Station Latitude", "Station Longitude", "Route1", "Route2", "Route3", "Route4", "Route5", "Route6", "Route7", "Route8", "Route9", "Route10", "Route11", "Entrance Type", "Entry", "Exit Only", "Vending", "Staffing", "Staff Hours", "ADA", "ADA Notes", "Free Crossover", "North South Street", "East West Street", "Corner", "Entrance Latitude", "Entrance Longitude", "Station Location", "Entrance Location" . For the data cleaning step, I choose to clean up variable names, which is convert all variable names to lower snake case. After that I use the select function to remain the needed variables, which are "line", "station_name", "station_latitude", "station_longitude", "Route1", "Route2", "Route3", "Route4", "Route5", "Route6", "Route7", "Route8", "Route9", "Route10", "Route11", "entry", "vending", "entrance_type" and "ada". The resulting dataset contains `r nrow(NYC_Transit)` row and `r ncol(NYC_Transit)` columns. These data are not tidy, because the variable route 1 to route 11 all means route but different route, so this data is not tidy.


```{r}
## distinct stations
distinct_stations <- 
  NYC_Transit %>% 
  distinct(station_name, line) %>%
   summarize(count = n())
```
There are `r distinct_stations$count` distinct stations.

```{r}
# Filter for ADA compliant stations (not distinct stations)
ada_compliant_stations_nd <- 
  NYC_Transit %>%
  filter(ada == "TRUE")

# Filter for ADA compliant stations (distinct stations)
ada_compliant_stations_d <- 
  NYC_Transit %>%
  filter(ada == "TRUE") %>%
  distinct(station_name, line)

# Count the number of non_distinct ADA compliant stations
num_nodistinct_ada_compliant_stations <- nrow(ada_compliant_stations_nd)

# Count the number of distinct ADA compliant stations
num_distinct_ada_compliant_stations <- nrow(ada_compliant_stations_d)
```

For the stations that are ADA compliant, we have 2 situations. One is for distinct stations, the other one is for the whole NYC transit dataset, which we do not find the number of the stations that are ADA compliant based on the distinct stations. For the number of the stations that are ADA compliant based on the non-distinct stations, we have `r num_nodistinct_ada_compliant_stations` stations, and for the number of the stations that are ADA compliant based on the distinct stations, we have `r num_distinct_ada_compliant_stations` stations.


```{r}
# Calculate the proportion of stations where vending is "NO" (non-distinct stations)
proportion_vending_no_nd <- 
  NYC_Transit %>%
  filter(vending == "NO") %>%
  summarize(proportion = mean(entry == "TRUE"))

# Calculate the proportion of stations where vending is "NO" (distinct stations)
distinct_stations <- 
  NYC_Transit %>% 
  distinct(station_name, line, .keep_all = TRUE)

proportion_vending_no_d <- 
  distinct_stations %>%
  filter(vending == "NO") %>%
  summarize(proportion = mean(entry == "TRUE"))
```

For the proportion of station entrances/exits without vending allow entrance, we have 2 situations. One is for distinct stations, the other one is for the whole NYC transit dataset, which we do not find the proportion of station entrances/exits without vending allow entrance based on the distinct stations. For the proportion of station entrances/exits without vending allow entrance based on the non-distinct stations, we have `r proportion_vending_no_nd$proportion` and for the proportion of station entrances/exits without vending allow entrance based on the distinct stations, we have `r proportion_vending_no_d$proportion`.


```{r}
NYC_Transit_long <- 
  NYC_Transit %>%
  mutate(across(route1:route11, as.character)) %>%
  pivot_longer(route1:route11, 
               names_to = "route_number", 
               values_to = "route") 

# Filter for stations that serve the A train and get distinct stations
a_train_stations <- NYC_Transit_long %>%
  filter(route == "A") %>%
  distinct(station_name, line, .keep_all = TRUE)

# Count the number of distinct stations that serve the A train
num_a_train_stations <- nrow(a_train_stations)
```
After reformate the data so that route number and route name are distinct variables, there are `r num_a_train_stations` distinct stations serve the A train. 

```{r}
# Filter for ADA compliant stations among those that serve the A train
ada_compliant_a_train_stations <- a_train_stations %>%
  filter(ada == "TRUE")

# Count the number of ADA compliant A train stations
num_ada_compliant_a_train_stations <- nrow(ada_compliant_a_train_stations)
```

Of the stations that serve the A train, there are `r num_ada_compliant_a_train_stations` stations have ADA compliant.

## Problem 2
```{r, warning=FALSE, message=FALSE}
# For Mr.Trash Wheel
# Specifying the sheet by name
Mr_Trash_Wheel <- read_excel("./202309 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", skip = 1)

# Rename columns to have reasonable variable names
colnames(Mr_Trash_Wheel) <- c("Dumpster", "Month", "Year", "Date", "Weight_tons", 
                                 "Volume_cubic_yards", "Plastic_Bottles", "Polystyrene", 
                                 "Cigarette_Butts", "Glass_Bottles", "Plastic_Bags", 
                                 "Wrappers", "Sports_Balls", "Homes_Powered", "Notes", "Extra")

# Remove columns containing notes (the last two columns 'Notes' and 'Extra')
Mr_Trash_Wheel <- 
  Mr_Trash_Wheel %>%
  select(-Notes, -Extra)

# Omit rows that do not contain dumpster-specific data (i.e., filter rows with non-NA values in 'Dumpster')
Mr_Trash_Wheel <- Mr_Trash_Wheel %>%
  filter(!is.na(Dumpster))

# Round the number of sports balls to the nearest integer and convert to integer
Mr_Trash_Wheel <- Mr_Trash_Wheel %>%
  mutate(Sports_Balls = as.integer(round(Sports_Balls)))
```

```{r}
# For Professor Trash Wheel
# Specifying the sheet by name
Professor_Trash_Wheel <- read_excel("./202309 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", skip = 1)

# Rename columns to have reasonable variable names
colnames(Professor_Trash_Wheel) <- c("Dumpster", "Month", "Year", "Date", "Weight_tons", 
                                 "Volume_cubic_yards", "Plastic_Bottles", "Polystyrene", 
                                 "Cigarette_Butts", "Glass_Bottles", "Plastic_Bags", 
                                 "Wrappers", "Homes_Powered")

# Omit rows that do not contain dumpster-specific data (i.e., filter rows with non-NA values in 'Dumpster')
Professor_Trash_Wheel <- Professor_Trash_Wheel %>%
  filter(!is.na(Dumpster))
```


```{r}
# For Gwynnda Trash Wheel
# Specifying the sheet by name
Gwynnda_Trash_Wheel <- read_excel("./202309 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", skip = 1)

# Rename columns to have reasonable variable names
colnames(Gwynnda_Trash_Wheel) <- c("Dumpster", "Month", "Year", "Date", "Weight_tons", 
                                 "Volume_cubic_yards", "Plastic_Bottles", "Polystyrene", 
                                 "Cigarette_Butts","Plastic_Bags", 
                                 "Wrappers", "Homes_Powered")

# Omit rows that do not contain dumpster-specific data (i.e., filter rows with non-NA values in 'Dumpster')
Gwynnda_Trash_Wheel <- Gwynnda_Trash_Wheel %>%
  filter(!is.na(Dumpster))
```

```{r}
# Add the Trash_Wheel column to each dataset to identify the source
Mr_Trash_Wheel <- Mr_Trash_Wheel %>%
  mutate(Trash_Wheel = "Mr. Trash Wheel")

Professor_Trash_Wheel <- Professor_Trash_Wheel %>%
  mutate(Trash_Wheel = "Professor Trash Wheel")

Gwynnda_Trash_Wheel <- Gwynnda_Trash_Wheel %>%
  mutate(Trash_Wheel = "Gwynnda Trash Wheel")
```

```{r}
# Ensure 'Year' is of the same type (e.g., character) in all datasets
Mr_Trash_Wheel <- Mr_Trash_Wheel %>%
  mutate(Year = as.character(Year))

Professor_Trash_Wheel <- Professor_Trash_Wheel %>%
  mutate(Year = as.character(Year))

Gwynnda_Trash_Wheel <- Gwynnda_Trash_Wheel %>%
  mutate(Year = as.character(Year))

# Combine datasets with bind_rows(), filling missing columns with NA
combined_trash_wheel_data <- bind_rows(Mr_Trash_Wheel, Professor_Trash_Wheel, Gwynnda_Trash_Wheel)
```

```{r}
# Filter for Professor Trash Wheel and calculate the total weight
total_weight_professor <- combined_trash_wheel_data %>%
  filter(Trash_Wheel == "Professor Trash Wheel") %>%
  summarize(total_weight = sum(Weight_tons, na.rm = TRUE))
```

```{r}
# Filter the data for Gwynnda Trash Wheel in June 2022 and sum the Cigarette Butts
total_cigarette_butts_gwynnda_june_2022 <- combined_trash_wheel_data %>%
  filter(Trash_Wheel == "Gwynnda Trash Wheel", 
         Month == "June", 
         Year == 2022) %>%
  summarize(total_cigarette_butts = sum(Cigarette_Butts, na.rm = TRUE))
```

After combining three dataset (Mr_Trash_Wheel, Professor_Trash_Wheel, Gwynnda_Trash_Wheel) to one single tidy dataset, we finally have `r nrow(combined_trash_wheel_data)` observations, key variables are like `r names(combined_trash_wheel_data)`. The total weight of trash collected by Professor Trash Wheel is `r total_weight_professor$total_weight` tons. The total number of cigarette butts collected by Gwynnda in June of 2022 is `r total_cigarette_butts_gwynnda_june_2022$total_cigarette_butts`.

## Problem 3

```{r, warning=FALSE, message=FALSE}
# read the dataset bakers, bakes and results
bakers <- read_csv(file = "./gbb_datasets/bakers.csv")
bakes <- read_csv(file = "./gbb_datasets/bakes.csv")
results <- read_csv(file = "./gbb_datasets/results.csv", skip = 2)

# clean up variable names
bakers = janitor::clean_names(bakers)
bakes = janitor::clean_names(bakes)
results = janitor::clean_names(results)
```

```{r}
# Extract first names from the full names in the bakers dataset
bakers <- bakers %>%
  mutate(baker_name = word(baker_name, 1)) 
```


```{r}
# Check if all bakers in results are present in bakers (using first_name)
missing_bakers <- anti_join(results, bakers, by = c("series", "baker" = "baker_name"))

# Check if all bakes in results are present in bakes
missing_bakes <- anti_join(results, bakes, by = c("series", "episode", "baker"))

# Check if all bakes in bakers are present in bakes
missing_bake_bakers <- anti_join(bakers, bakes, by = c("series", "baker_name"="baker"))

# Correct a typo in the baker name (Jo-Joanne) --bakers dataset
bakers <- bakers %>%
  mutate(baker_name = ifelse(baker_name == "Jo", "Joanne", baker_name))  # Correcting "Jo" to "Joanne"

# Correct a typo in the baker name ("Jo"-Joanne) --bakes dataset
bakes <- bakes %>%
  mutate(baker = ifelse(baker == '"Jo"', "Joanne", baker))  # Correcting "Jo" to "Joanne"

# Check if all bakers in results are present in bakers (using first_name) again
missing_bakers <- anti_join(results, bakers, by = c("series", "baker" = "baker_name"))

# Check if all bakes in results are present in bakes again
missing_bakes <- anti_join(results, bakes, by = c("series", "episode", "baker"))

# Merge results with bakers and bakes
merged_data <- results %>%
  left_join(bakers, by = c("series", "baker" = "baker_name")) %>%
  left_join(bakes, by = c("series", "episode", "baker"))

```

```{r}
# Export the final dataset to a CSV file
write_csv(merged_data, "./final_bake_off_data.csv")
```

The data cleaning process involved several key steps to ensure the bakers, bakes, and results datasets were merged into a single, well-organized dataset. Initially, all three datasets were imported to understand their structure. One of the first tasks was to standardize column names across all datasets using the janitor::clean_names() function, converting all names to a consistent snake_case format. 

A notable challenge was that the bakers dataset contained full names, while the bakes and results datasets only included first names. To resolve this, I extracted the first names from the full names in the bakers dataset, ensuring that they would align with the other datasets during the merging process. I also used anti_join() to check for any missing or mismatched data between the datasets. In the dataset "bakers", I found there is a baker named "Jo" in series 2 and episode 1-8, and there is a baker in the dataset results called "Joanne" also appears in series 2 episode 1-8, so I think they are same person, so I choose to mutate the name of "Jo"to the "Joanne", and in the dataset "bakes", I use the "Joanne" to replease the '"Jo"'. Also, there is no series 9 and series 10 in the dataset bakes, so when I use the antijoin between bake and bakers, it appears all bakers in series 9 and 10. Therefore, I will choose to keep them for the completeness. However, when I antijoin results and the bakes, I found there are some bakers appears in "results" but not in the dataset "bakes".

After ensuring that all datasets were consistent and clean, I merged them using left_join() based on shared keys such as series, episode, and baker. Following the merge, I reorganized the columns so that key variables, such as baker demographics, bake descriptions, and results, were easy to access. The final dataset was then exported as a CSV file for further analysis.

The final dataset contains comprehensive information about the bakers, their bakes, and their performance across different episodes. This clean dataset is now ready for analysis and can be used to explore patterns in performance, baking success, and eliminations throughout the competition.

```{r}
# Filter data for Seasons 5 through 10 and episodes with a Star Baker or Winner
star_baker_winner <- merged_data %>%
  filter(series >= 5 & series <= 10, result %in% c("STAR BAKER", "WINNER")) %>%
  select(series, episode, baker, result) %>%
  arrange(series, episode)

# View the reader-friendly table of Star Bakers and Winners for Seasons 5-10
print(star_baker_winner)
```
There are `r nrow(star_baker_winner)` Star Bakers and Winners for Seasons 5-10. In this table, I see the list of bakers who either earned Star Baker or became the Winner of each episode from Seasons 5 through 10. From the table, we can find that there were some predictable overall winners, and there were also some suprised overall winners. It is beacuse some bakers are winners or star bakers more than 1 times, and some of them only one time. For example in series 6, a baker named Nadiya wins 3 star bakers and 1 winners, so this will be predictable. 

```{r, warning=FALSE, message=FALSE}
# read the viewship data
viewship <- read_csv(file = "./gbb_datasets/viewers.csv")
```

```{r}
# Convert the dataset from wide format to long format
viewership_long <- viewship %>%
  pivot_longer(cols = starts_with("Series"), 
               names_to = "series", 
               names_prefix = "Series ", 
               values_to = "viewership") %>%
  mutate(series = as.numeric(series))  # Convert series to numeric for calculations

# Show the first 10 rows of the cleaned dataset
head(viewership_long, 10)
```

```{r}
# Calculate average viewership for Season 1
average_viewership_season1 <- viewership_long %>%
  filter(series == 1) %>%
  summarize(average_viewership = mean(viewership, na.rm = TRUE))

# Calculate average viewership for Season 5
average_viewership_season5 <- viewership_long %>%
  filter(series == 5) %>%
  summarize(average_viewership = mean(viewership, na.rm = TRUE))
```
The average viewship for season1 (series1) is `r average_viewership_season1$average_viewership`, and the average viewship for season5 (series5) is `r average_viewership_season5$average_viewership`.