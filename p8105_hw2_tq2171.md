p8105_hw2_tq2171
================
Tingyu Qian
2024-09-29

# Problem 1

``` r
# read the package
library(tidyverse) 
library(dplyr)
library(tidyr)
library(readxl)
```

``` r
## read and clean the data
NYC_Transit = read_csv (file = "./NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA", ".")) #read the data
NYC_Transit = janitor::clean_names(NYC_Transit) #clean up variable names after importing data, this will take whatever the column names are and convert them to lower snake case.
```

``` r
## retain the needed variables
NYC_Transit<-
  select(NYC_Transit, line, station_name, station_latitude, station_longitude, route1:route11, entry, vending, entrance_type, ada)
```

``` r
## convert the entry variable from character to a logical variable
NYC_Transit$entry <- ifelse(NYC_Transit$entry == "YES", TRUE, FALSE)
```

In the original dataset NYC_Transit, it contains 1868 and 32 columns,
which means there are 32 variables. These variables are “Division”,
“Line”, “Station Name”, “Station Latitude”, “Station Longitude”,
“Route1”, “Route2”, “Route3”, “Route4”, “Route5”, “Route6”, “Route7”,
“Route8”, “Route9”, “Route10”, “Route11”, “Entrance Type”, “Entry”,
“Exit Only”, “Vending”, “Staffing”, “Staff Hours”, “ADA”, “ADA Notes”,
“Free Crossover”, “North South Street”, “East West Street”, “Corner”,
“Entrance Latitude”, “Entrance Longitude”, “Station Location”, “Entrance
Location” . For the data cleaning step, I choose to clean up variable
names, which is convert all variable names to lower snake case. After
that I use the select function to remain the needed variables, which are
“line”, “station_name”, “station_latitude”, “station_longitude”,
“Route1”, “Route2”, “Route3”, “Route4”, “Route5”, “Route6”, “Route7”,
“Route8”, “Route9”, “Route10”, “Route11”, “entry”, “vending”,
“entrance_type” and “ada”. The resulting dataset contains 1868 row and
19 columns. These data are not tidy, because the variable route 1 to
route 11 all means route but different route, so this data is not tidy.

``` r
## distinct stations
distinct_stations <- 
  NYC_Transit %>% 
  distinct(station_name, line) %>%
   summarize(count = n())
```

There are 465 distinct stations.

``` r
# Filter for ADA compliant stations (not distinct stations)
ada_compliant_stations_nd <- 
  NYC_Transit %>%
  filter(ada == "TRUE")

# Filter for ADA compliant stations (distinct stations)
ada_compliant_stations_d <- 
  NYC_Transit %>%
  filter(ada == "TRUE") %>%
  distinct(station_name, line)

# Count the number of non_distinct ADA compliant stations
num_nodistinct_ada_compliant_stations <- nrow(ada_compliant_stations_nd)

# Count the number of distinct ADA compliant stations
num_distinct_ada_compliant_stations <- nrow(ada_compliant_stations_d)
```

For the stations that are ADA compliant, we have 2 situations. One is
for distinct stations, the other one is for the whole NYC transit
dataset, which we do not find the number of the stations that are ADA
compliant based on the distinct stations. For the number of the stations
that are ADA compliant based on the non-distinct stations, we have 468
stations, and for the number of the stations that are ADA compliant
based on the distinct stations, we have 84 stations.

``` r
# Calculate the proportion of stations where vending is "NO" (non-distinct stations)
proportion_vending_no_nd <- 
  NYC_Transit %>%
  filter(vending == "NO") %>%
  summarize(proportion = mean(entry == "TRUE"))

# Calculate the proportion of stations where vending is "NO" (distinct stations)
distinct_stations <- 
  NYC_Transit %>% 
  distinct(station_name, line, .keep_all = TRUE)

proportion_vending_no_d <- 
  distinct_stations %>%
  filter(vending == "NO") %>%
  summarize(proportion = mean(entry == "TRUE"))
```

For the proportion of station entrances/exits without vending allow
entrance, we have 2 situations. One is for distinct stations, the other
one is for the whole NYC transit dataset, which we do not find the
proportion of station entrances/exits without vending allow entrance
based on the distinct stations. For the proportion of station
entrances/exits without vending allow entrance based on the non-distinct
stations, we have 0.3770492 and for the proportion of station
entrances/exits without vending allow entrance based on the distinct
stations, we have 0.5555556.

``` r
NYC_Transit_long <- 
  NYC_Transit %>%
  mutate(across(route1:route11, as.character)) %>%
  pivot_longer(route1:route11, 
               names_to = "route_number", 
               values_to = "route") 

# Filter for stations that serve the A train and get distinct stations
a_train_stations <- NYC_Transit_long %>%
  filter(route == "A") %>%
  distinct(station_name, line, .keep_all = TRUE)

# Count the number of distinct stations that serve the A train
num_a_train_stations <- nrow(a_train_stations)
```

After reformate the data so that route number and route name are
distinct variables, there are 60 distinct stations serve the A train.

``` r
# Filter for ADA compliant stations among those that serve the A train
ada_compliant_a_train_stations <- a_train_stations %>%
  filter(ada == "TRUE")

# Count the number of ADA compliant A train stations
num_ada_compliant_a_train_stations <- nrow(ada_compliant_a_train_stations)
```

Of the stations that serve the A train, there are 17 stations have ADA
compliant.

## Problem 2

``` r
# For Mr.Trash Wheel
# Specifying the sheet by name
Mr_Trash_Wheel <- read_excel("./202309 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel", skip = 1)

# Rename columns to have reasonable variable names
colnames(Mr_Trash_Wheel) <- c("Dumpster", "Month", "Year", "Date", "Weight_tons", 
                                 "Volume_cubic_yards", "Plastic_Bottles", "Polystyrene", 
                                 "Cigarette_Butts", "Glass_Bottles", "Plastic_Bags", 
                                 "Wrappers", "Sports_Balls", "Homes_Powered", "Notes", "Extra")

# Remove columns containing notes (the last two columns 'Notes' and 'Extra')
Mr_Trash_Wheel <- 
  Mr_Trash_Wheel %>%
  select(-Notes, -Extra)

# Omit rows that do not contain dumpster-specific data (i.e., filter rows with non-NA values in 'Dumpster')
Mr_Trash_Wheel <- Mr_Trash_Wheel %>%
  filter(!is.na(Dumpster))

# Round the number of sports balls to the nearest integer and convert to integer
Mr_Trash_Wheel <- Mr_Trash_Wheel %>%
  mutate(Sports_Balls = as.integer(round(Sports_Balls)))
```

``` r
# For Professor Trash Wheel
# Specifying the sheet by name
Professor_Trash_Wheel <- read_excel("./202309 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", skip = 1)

# Rename columns to have reasonable variable names
colnames(Professor_Trash_Wheel) <- c("Dumpster", "Month", "Year", "Date", "Weight_tons", 
                                 "Volume_cubic_yards", "Plastic_Bottles", "Polystyrene", 
                                 "Cigarette_Butts", "Glass_Bottles", "Plastic_Bags", 
                                 "Wrappers", "Homes_Powered")

# Omit rows that do not contain dumpster-specific data (i.e., filter rows with non-NA values in 'Dumpster')
Professor_Trash_Wheel <- Professor_Trash_Wheel %>%
  filter(!is.na(Dumpster))
```

``` r
# For Gwynnda Trash Wheel
# Specifying the sheet by name
Gwynnda_Trash_Wheel <- read_excel("./202309 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", skip = 1)

# Rename columns to have reasonable variable names
colnames(Gwynnda_Trash_Wheel) <- c("Dumpster", "Month", "Year", "Date", "Weight_tons", 
                                 "Volume_cubic_yards", "Plastic_Bottles", "Polystyrene", 
                                 "Cigarette_Butts","Plastic_Bags", 
                                 "Wrappers", "Homes_Powered")

# Omit rows that do not contain dumpster-specific data (i.e., filter rows with non-NA values in 'Dumpster')
Gwynnda_Trash_Wheel <- Gwynnda_Trash_Wheel %>%
  filter(!is.na(Dumpster))
```

``` r
# Add the Trash_Wheel column to each dataset to identify the source
Mr_Trash_Wheel <- Mr_Trash_Wheel %>%
  mutate(Trash_Wheel = "Mr. Trash Wheel")

Professor_Trash_Wheel <- Professor_Trash_Wheel %>%
  mutate(Trash_Wheel = "Professor Trash Wheel")

Gwynnda_Trash_Wheel <- Gwynnda_Trash_Wheel %>%
  mutate(Trash_Wheel = "Gwynnda Trash Wheel")
```

``` r
# Ensure 'Year' is of the same type (e.g., character) in all datasets
Mr_Trash_Wheel <- Mr_Trash_Wheel %>%
  mutate(Year = as.character(Year))

Professor_Trash_Wheel <- Professor_Trash_Wheel %>%
  mutate(Year = as.character(Year))

Gwynnda_Trash_Wheel <- Gwynnda_Trash_Wheel %>%
  mutate(Year = as.character(Year))

# Combine datasets with bind_rows(), filling missing columns with NA
combined_trash_wheel_data <- bind_rows(Mr_Trash_Wheel, Professor_Trash_Wheel, Gwynnda_Trash_Wheel)
```

``` r
# Filter for Professor Trash Wheel and calculate the total weight
total_weight_professor <- combined_trash_wheel_data %>%
  filter(Trash_Wheel == "Professor Trash Wheel") %>%
  summarize(total_weight = sum(Weight_tons, na.rm = TRUE))
```

``` r
# Filter the data for Gwynnda Trash Wheel in June 2022 and sum the Cigarette Butts
total_cigarette_butts_gwynnda_june_2022 <- combined_trash_wheel_data %>%
  filter(Trash_Wheel == "Gwynnda Trash Wheel", 
         Month == "June", 
         Year == 2022) %>%
  summarize(total_cigarette_butts = sum(Cigarette_Butts, na.rm = TRUE))
```

After combining three dataset (Mr_Trash_Wheel, Professor_Trash_Wheel,
Gwynnda_Trash_Wheel) to one single tidy dataset, we finally have 845
observations, key variables are like Dumpster, Month, Year, Date,
Weight_tons, Volume_cubic_yards, Plastic_Bottles, Polystyrene,
Cigarette_Butts, Glass_Bottles, Plastic_Bags, Wrappers, Sports_Balls,
Homes_Powered, Trash_Wheel. The total weight of trash collected by
Professor Trash Wheel is 216.26 tons. The total number of cigarette
butts collected by Gwynnda in June of 2022 is 1.812^{4}.

## Problem 3

``` r
# read the dataset bakers, bakes and results
bakers <- read_csv(file = "./gbb_datasets/bakers.csv")
bakes <- read_csv(file = "./gbb_datasets/bakes.csv")
results <- read_csv(file = "./gbb_datasets/results.csv", skip = 2)

# clean up variable names
bakers = janitor::clean_names(bakers)
bakes = janitor::clean_names(bakes)
results = janitor::clean_names(results)
```

``` r
# Extract first names from the full names in the bakers dataset
bakers <- bakers %>%
  mutate(baker_name = word(baker_name, 1)) 
```

``` r
# Check if all bakers in results are present in bakers (using first_name)
missing_bakers <- anti_join(results, bakers, by = c("series", "baker" = "baker_name"))

# Check if all bakes in results are present in bakes
missing_bakes <- anti_join(results, bakes, by = c("series", "episode", "baker"))

# Check if all bakes in bakers are present in bakes
missing_bake_bakers <- anti_join(bakers, bakes, by = c("series", "baker_name"="baker"))

# Correct a typo in the baker name (Jo-Joanne) --bakers dataset
bakers <- bakers %>%
  mutate(baker_name = ifelse(baker_name == "Jo", "Joanne", baker_name))  # Correcting "Jo" to "Joanne"

# Correct a typo in the baker name ("Jo"-Joanne) --bakes dataset
bakes <- bakes %>%
  mutate(baker = ifelse(baker == '"Jo"', "Joanne", baker))  # Correcting "Jo" to "Joanne"

# Check if all bakers in results are present in bakers (using first_name) again
missing_bakers <- anti_join(results, bakers, by = c("series", "baker" = "baker_name"))

# Check if all bakes in results are present in bakes again
missing_bakes <- anti_join(results, bakes, by = c("series", "episode", "baker"))

# Merge results with bakers and bakes
merged_data <- results %>%
  left_join(bakers, by = c("series", "baker" = "baker_name")) %>%
  left_join(bakes, by = c("series", "episode", "baker"))
```

``` r
# Export the final dataset to a CSV file
write_csv(merged_data, "./final_bake_off_data.csv")
```

The data cleaning process involved several key steps to ensure the
bakers, bakes, and results datasets were merged into a single,
well-organized dataset. Initially, all three datasets were imported to
understand their structure. One of the first tasks was to standardize
column names across all datasets using the janitor::clean_names()
function, converting all names to a consistent snake_case format.

A notable challenge was that the bakers dataset contained full names,
while the bakes and results datasets only included first names. To
resolve this, I extracted the first names from the full names in the
bakers dataset, ensuring that they would align with the other datasets
during the merging process. I also used anti_join() to check for any
missing or mismatched data between the datasets. In the dataset
“bakers”, I found there is a baker named “Jo” in series 2 and episode
1-8, and there is a baker in the dataset results called “Joanne” also
appears in series 2 episode 1-8, so I think they are same person, so I
choose to mutate the name of “Jo”to the “Joanne”, and in the dataset
“bakes”, I use the “Joanne” to replease the ‘“Jo”’. Also, there is no
series 9 and series 10 in the dataset bakes, so when I use the antijoin
between bake and bakers, it appears all bakers in series 9 and 10.
Therefore, I will choose to keep them for the completeness. However,
when I antijoin results and the bakes, I found there are some bakers
appears in “results” but not in the dataset “bakes”.

After ensuring that all datasets were consistent and clean, I merged
them using left_join() based on shared keys such as series, episode, and
baker. Following the merge, I reorganized the columns so that key
variables, such as baker demographics, bake descriptions, and results,
were easy to access. The final dataset was then exported as a CSV file
for further analysis.

The final dataset contains comprehensive information about the bakers,
their bakes, and their performance across different episodes. This clean
dataset is now ready for analysis and can be used to explore patterns in
performance, baking success, and eliminations throughout the
competition.

``` r
# Filter data for Seasons 5 through 10 and episodes with a Star Baker or Winner
star_baker_winner <- merged_data %>%
  filter(series >= 5 & series <= 10, result %in% c("STAR BAKER", "WINNER")) %>%
  select(series, episode, baker, result) %>%
  arrange(series, episode)

# View the reader-friendly table of Star Bakers and Winners for Seasons 5-10
print(star_baker_winner)
```

    ## # A tibble: 60 × 4
    ##    series episode baker   result    
    ##     <dbl>   <dbl> <chr>   <chr>     
    ##  1      5       1 Nancy   STAR BAKER
    ##  2      5       2 Richard STAR BAKER
    ##  3      5       3 Luis    STAR BAKER
    ##  4      5       4 Richard STAR BAKER
    ##  5      5       5 Kate    STAR BAKER
    ##  6      5       6 Chetna  STAR BAKER
    ##  7      5       7 Richard STAR BAKER
    ##  8      5       8 Richard STAR BAKER
    ##  9      5       9 Richard STAR BAKER
    ## 10      5      10 Nancy   WINNER    
    ## # ℹ 50 more rows

There are 60 Star Bakers and Winners for Seasons 5-10. In this table, I
see the list of bakers who either earned Star Baker or became the Winner
of each episode from Seasons 5 through 10. From the table, we can find
that there were some predictable overall winners, and there were also
some suprised overall winners. It is beacuse some bakers are winners or
star bakers more than 1 times, and some of them only one time. For
example in series 6, a baker named Nadiya wins 3 star bakers and 1
winners, so this will be predictable.

``` r
# read the viewship data
viewship <- read_csv(file = "./gbb_datasets/viewers.csv")
```

``` r
# Convert the dataset from wide format to long format
viewership_long <- viewship %>%
  pivot_longer(cols = starts_with("Series"), 
               names_to = "series", 
               names_prefix = "Series ", 
               values_to = "viewership") %>%
  mutate(series = as.numeric(series))  # Convert series to numeric for calculations

# Show the first 10 rows of the cleaned dataset
head(viewership_long, 10)
```

    ## # A tibble: 10 × 3
    ##    Episode series viewership
    ##      <dbl>  <dbl>      <dbl>
    ##  1       1      1       2.24
    ##  2       1      2       3.1 
    ##  3       1      3       3.85
    ##  4       1      4       6.6 
    ##  5       1      5       8.51
    ##  6       1      6      11.6 
    ##  7       1      7      13.6 
    ##  8       1      8       9.46
    ##  9       1      9       9.55
    ## 10       1     10       9.62

``` r
# Calculate average viewership for Season 1
average_viewership_season1 <- viewership_long %>%
  filter(series == 1) %>%
  summarize(average_viewership = mean(viewership, na.rm = TRUE))

# Calculate average viewership for Season 5
average_viewership_season5 <- viewership_long %>%
  filter(series == 5) %>%
  summarize(average_viewership = mean(viewership, na.rm = TRUE))
```

The average viewship for season1 (series1) is 2.77, and the average
viewship for season5 (series5) is 10.0393.
